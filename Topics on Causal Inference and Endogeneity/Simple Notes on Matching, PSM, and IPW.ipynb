{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We are already familiar with the definition and calculation of propensity score\n",
    "$$ps = pr(D=1|x)$$\n",
    "It is applied to solve the problem of high-dimension data problem during matching. \n",
    "\n",
    "## Using Propensity Score: PSM\n",
    "For each individual $i$ in the treatment group, we find the samples in the control group with the same or close $ps$ with $i$. By this procedure, we actually create a pseudo control group, whose size is the same as in treatment group. We can use this newly generated data to do regression or non-parametric estimation. We only need to condition on $ps(x)$, instead of the high-dimensional $x$.\n",
    "\n",
    "### Demerit\n",
    "Creating a control group by matching has the distressing side-effect of throwing away large amounts of the data, because the control group is shrunk down to the same size as the treatment group. This happens especially when the characteristics of groups are too different\n",
    "\n",
    "## Using Propensity Score: IPW \n",
    "For each $x$ value($x$ may be high dimensional), we can get $ps(x)$. For a given $x$, we have some inviduals in the treatment group, and some in the control group. Consider for example $ps(x) = 0.1$, a low probability of entering the treatment. Naturally, for this $x$, the number of individuals in treatment group is much smaller than that of the individuals in control group. This is as if each individuals in the treatment is more 'important' than those in the control group, so they should be given a 'higher weight', which is exactly the inverse of $ps(x)$. Theindividuals in control group is given a 'lower weight', which is exactly the inverse of $(1-ps(x))$.\n",
    "\n",
    "Once finishing this data construction, we can again use regression or non-parametric estimation. The merit of IPW is that it keeps all the data information. For example, for non-parametric methods, we can calculate \n",
    "$$ATE = E_x\\left(\\frac{1(D=1)y}{ps(x)}\\right)- E_x\\left(\\frac{1(D=0)y}{1-ps(x)}\\right)$$\n",
    "\n",
    "\n",
    "### Demerit\n",
    "One of the criticisms of this inverse probability of treatment weighting approach is that individual observations can get very high weights and become unduly influential.Consider a lone treated observation that happens to have a very low probability of being treated. The value of the inverse of the propensity score will be extremely high, asymptotically infinity. The effect size obtained will be dominated by this single value, and any fluctuations in it will produce wildly varied results, which is an undesirable property.\n",
    "\n",
    "## Common issues with PS:\n",
    " The predictive quality of the propensity score does not translate into its balancing properties.Maximising the prediction power of the propensity score can even hurt the causal inference goal. Propensity score doesn’t need to predict the treatment very well. It just needs to <b>include all the confounding variables</b>. \n",
    " \n",
    " If we include variables that are very good in predicting the treatment but have no bearing on the outcome this will actually increase the variance of the propensity score estimator. This is similar to the problem linear regression faces when we include variables correlated with the treatment but not with the outcome.\n",
    "\n",
    "## Doubly Robust \n",
    "\n",
    "We already know how to estimate $ATE(x)$ using nonparmetric method or linear regression. Which one should we use? When in doubt, just use both! Doubly Robust Estimation is a way of combining propensity score and linear regression in a way you don’t have to rely on either of them. See [Here](https://matheusfacure.github.io/python-causality-handbook/12-Doubly-Robust-Estimation.html) for a detailed expression for doubly robust estimation.\n",
    "\n",
    "## Reference:\n",
    "- [Cross Validate](https://stats.stackexchange.com/questions/293960/how-does-inverse-weighted-propensity-score-regression-differ-from-propensity-sco/421200)\n",
    "- [A Careful Guide 1](http://freerangestats.info/blog/2017/04/09/propensity-v-regression)\n",
    "- [A Careful Guide 2](https://matheusfacure.github.io/python-causality-handbook/11-Propensity-Score.html)\n",
    "- [A Youtube Guide](https://www.youtube.com/watch?v=VJhLaOdpUv0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
